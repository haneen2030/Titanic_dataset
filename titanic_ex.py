# -*- coding: utf-8 -*-
"""Titanic_ex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TEYfbvt_3nZ6PuieiI6SwtzvGlFm9Vmt

# Titanic - Machine Learning

This project aims to introduce the most important steps of data analysis and explore the different stages. We will use the data of Titanic survivors available on the Kaggle website at the following link:
https://www.kaggle.com/competitions/titanic/overview

You can download the dataset and explore all the information about it in the following link: https://www.kaggle.com/competitions/titanic/data

# Importing the dependncies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""# Reading the data

"""

data = pd.read_csv('titanic.csv')

data.head()

data.shape

data.describe()

"""# Data Preprocessing  """

data.info()

"""# Dealing with messing data

Three options to fix:
Choose the suitable one for each case.

*   Delete rows that contains missing values.
*   Delete the whole column thats contains missing values.
*   Replace missing values with some values(mean,median,mode,constant)
"""

data.isnull().sum()

"""There are three columns contains Missing values: **Age, Cabin, Embarked**. <br>
In the Age column, we will fill the missing values with the mean since it is a simple and quick method to handle missing data and helps maintain the overall distribution of the dataset.
"""

#fill the missing values in Age with the mean of Age column
#you can simply use 'filllna' function, or any other way such as SimpleImputer
data['Age'].fillna(data['Age'].mean(), inplace=True)

data['Age'].isnull().sum()

"""There are a large number of missing values in the Cabin column, so we will drop this column from the dataset."""

data.drop(['Cabin'], axis=1, inplace=True)

data.head()

"""In the Embarked column, there are only two missing values. Let's see what the categories in this column are."""

data['Embarked'].unique()

data['Embarked'].value_counts()

data['Embarked'].mode()[0]

data['Embarked'].fillna('S', inplace=True)

data['Embarked'].isnull().sum()

"""# Drop useless columns

As you know, the PassengerId and Name of the Passenger do not affect the probability of survival. and ticket column does not have a clear relationship to the survival of passengers, so they will be dropped:
"""

data = data.drop(['PassengerId','Name','Ticket'], axis=1)

data.head()

"""sibsp: 	# of siblings / spouses aboard the Titanic
parch: 	# of parents / children aboard the Titanic
"""

data['SibSp'].value_counts()

data['Parch'].value_counts()

data['family'] = data['SibSp']+data['Parch']
data.drop(['SibSp','Parch'], axis=1,inplace=True)

data.head()

"""# Dealing with duplicates"""

data.duplicated().sum()

data.drop_duplicates(inplace=True)

"""# Encode categorical columns

Sex and Embarked columns values are text, we can't give this text directly to the machine learning model, so we need to replace this text values to meaningful numerical values.

In Age column we will replace all male values with 1 and all the female values with 0. <br>
and we will do the same in Embarked column: S, C, Q
"""

data.replace({'Sex':{'male':0,'female':1}}, inplace=True)

data.head()

data['Embarked'].replace({'S':1,'C':2,'Q':3}, inplace=True)

data.head()

"""# Data anylysis

# Split the dataset

**Separating features & Target** <br><br>
Separating features and target so that we can prepare the data for training machine learning models. In the Titanic dataset, the Survived column is the target variable, and the other columns are the features.
"""

x = data.drop(columns = ['Survived'], axis=1)
y = data['Survived']

"""**Splitting the data into training data & Testing data**

To build and evaluate a machine learning model effectively, it's essential to split the dataset into training and testing sets. The training set is used to train the model, allowing it to learn patterns and relationships within the data. The testing set, on the other hand, is used to evaluate the model's performance on unseen data, ensuring it can generalize well to new instances. This split helps prevent overfitting and provides a reliable estimate of the model's predictive accuracy.
"""

from sklearn.model_selection import train_test_split

# Split the data into training data & Testing data using train_test_split function :
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

data.head()

"""# Normailization"""

data.head()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

x_train = scaler.fit_transform(x_train)

x_test = scaler.transform(x_test)

"""# Model Training

Model training is a crucial step in the machine learning where the algorithm learns from the training data to make predictions. **Logistic Regression** is a commonly used algorithm for binary classification tasks, such as predicting whether a passenger survived in the Titanic dataset. By training the model on our training data, we aim to find the best-fit parameters that minimize prediction errors. Once trained, this model can be used to predict outcomes on new, unseen data.
"""

from sklearn.linear_model import LogisticRegression

# Create a Logistic Regression model and Train it on the training data:

model = LogisticRegression()
model.fit(x_train, y_train)

"""# Model Evaluation

Model evaluation is crucial in machine learning to assess the performance of a trained model on testing data. The **accuracy score**, a common evaluation metric, measures the proportion of correct predictions out of all predictions. This helps to gauge the model's effectiveness, ensure it generalizes well to new data, and guide further improvements.
"""

from sklearn.metrics import accuracy_score

# accuracy on testing data
x_test_prediction = model.predict(x_test)
test_data_accuracy = accuracy_score(y_test, x_test_prediction)
print('Accuracy score of training data : ', test_data_accuracy)